{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmniFold Demo CLAS6\n",
    "\n",
    "In this tutorial, we showcase the OmniFold method for universally unfolding collider data using synthetic datasets of jets.\n",
    "\n",
    "Unfolding is the problem of estimating the particle-level (`truth`, this is basically what we want to find for our date) information from the measured detector-level data (`data`, the measurement). The detector is imperfect and smears the particle-level radiation pattern, giving rise to the need to unfold. OmniFold uses a synthetic dataset where the particle-level (`generation`) and detector-level (`simulation`) information are both known.\n",
    "\n",
    "OmniFold is an iterative unfolding procedure consisting of two steps.\n",
    "* First, the `simulation` is reweighted to the `data`.\n",
    "* Second, the previous `generation` is reweighted to the new `generation`.\n",
    "\n",
    "OmniFold results in a set of weights for the `generation` that reweight it to an estimate for the `truth` distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 15:12:11.315495: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-27 15:12:12.046290: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/matias/software/build/lib:/home/matias/software/ClasTool/slib/Linux:/home/matias/software/Analyser/slib:\n",
      "2022-10-27 15:12:12.046395: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-27 15:12:14.965488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/matias/software/build/lib:/home/matias/software/ClasTool/slib/Linux:/home/matias/software/Analyser/slib:\n",
      "2022-10-27 15:12:14.966215: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/matias/software/build/lib:/home/matias/software/ClasTool/slib/Linux:/home/matias/software/Analyser/slib:\n",
      "2022-10-27 15:12:14.966243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import energyflow as ef\n",
    "import energyflow.archs\n",
    "\n",
    "import omnifold\n",
    "import modplot\n",
    "import ibu\n",
    "import uproot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4,4)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.family'] = 'serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Datasets\n",
    "\n",
    "Here I use half of the Hayk's simulations as the syntetic events(Simulation to train the algotith) and the other half as the nature events (this will be the data in the real case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory where the simulations is storeded\n",
    "data_directory  = \"/home/matias/proyecto/Omnifold/Data/\"\n",
    "target = \"C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the variables of the space in which is done the correction, you must have the Generated values(gen) \n",
    "# and the reconstructed/detected values(rec) \n",
    "vars = ['Gen', 'Q2_gen', 'Nu_gen', 'Pt2_gen', 'Zh_gen', 'Pt2_gen', 'PhiPQ_gen',\n",
    "        'Rec', 'Q2_rec', 'Nu_rec', 'Pt2_rec', 'Zh_rec', 'Pt2_rec', 'PhiPQ_rec']\n",
    "dummyval = -999.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Open the files and saves the variables in dictionaries \n",
    "with uproot.open(data_directory + \"OF_SIM_\" + target + \"_1.root:ntuple_sim\") as file:\n",
    "    sim = file.arrays(vars, library = \"np\")\n",
    "with uproot.open(data_directory + \"OF_SIM_\" + target + \"_2.root:ntuple_sim\") as file:\n",
    "    data = file.arrays(vars, library = \"np\")\n",
    "print(len(data['Q2_gen']))\n",
    "print(len(sim['Q2_gen']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A dictionary of the dictionaries \n",
    "datasets = {'simul':sim,'data':data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'simul': {'Gen': array([1., 1., 1., ..., 1., 1., 1.], dtype=float32), 'Q2_gen': array([2.2373629, 2.2373629, 1.1274263, ..., 1.2646748, 1.2646748,\n",
      "       1.1104889], dtype=float32), 'Nu_gen': array([3.3648884, 3.3648884, 2.3245723, ..., 4.3668857, 4.3668857,\n",
      "       2.6172612], dtype=float32), 'Pt2_gen': array([0.00332313, 0.01568869, 0.02776355, ..., 0.01283637, 0.52805233,\n",
      "       0.5158498 ], dtype=float32), 'Zh_gen': array([0.13696966, 0.19608033, 0.14992525, ..., 0.04117944, 0.25569254,\n",
      "       0.8251546 ], dtype=float32), 'PhiPQ_gen': array([ -24.517366 ,  140.52121  ,    1.0800816, ..., -154.82985  ,\n",
      "        -82.68674  ,  135.3871   ], dtype=float32), 'Rec': array([0., 0., 0., ..., 0., 0., 1.], dtype=float32), 'Q2_rec': array([-999.       , -999.       , -999.       , ..., -999.       ,\n",
      "       -999.       ,    1.1166188], dtype=float32), 'Nu_rec': array([-999.       , -999.       , -999.       , ..., -999.       ,\n",
      "       -999.       ,    2.5994642], dtype=float32), 'Pt2_rec': array([-9.990000e+02, -9.990000e+02, -9.990000e+02, ..., -9.990000e+02,\n",
      "       -9.990000e+02,  5.067523e-01], dtype=float32), 'Zh_rec': array([-9.990000e+02, -9.990000e+02, -9.990000e+02, ..., -9.990000e+02,\n",
      "       -9.990000e+02,  8.300132e-01], dtype=float32), 'PhiPQ_rec': array([-999.     , -999.     , -999.     , ..., -999.     , -999.     ,\n",
      "        135.29865], dtype=float32)}, 'data': {'Gen': array([1., 1., 1., ..., 1., 1., 1.], dtype=float32), 'Q2_gen': array([2.8307369, 1.1335685, 2.8496184, ..., 1.0232067, 1.0232067,\n",
      "       1.895879 ], dtype=float32), 'Nu_gen': array([3.8737168, 2.5969598, 4.6378336, ..., 3.798965 , 3.798965 ,\n",
      "       2.6989415], dtype=float32), 'Pt2_gen': array([0.1392336 , 0.15775338, 0.00396976, ..., 0.05208748, 0.03494513,\n",
      "       0.43397808], dtype=float32), 'Zh_gen': array([0.22505255, 0.9130801 , 0.41318566, ..., 0.07119108, 0.35539162,\n",
      "       0.6534634 ], dtype=float32), 'PhiPQ_gen': array([ 101.88023 ,  -55.551685, -145.88    , ..., -171.86372 ,\n",
      "        -60.48616 ,  -47.2496  ], dtype=float32), 'Rec': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'Q2_rec': array([-999., -999., -999., ..., -999., -999., -999.], dtype=float32), 'Nu_rec': array([-999., -999., -999., ..., -999., -999., -999.], dtype=float32), 'Pt2_rec': array([-999., -999., -999., ..., -999., -999., -999.], dtype=float32), 'Zh_rec': array([-999., -999., -999., ..., -999., -999., -999.], dtype=float32), 'PhiPQ_rec': array([-999., -999., -999., ..., -999., -999., -999.], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "# if you want to check the format run this\n",
    "print(datasets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the Unfolding Problem\n",
    "\n",
    "OmniFold requires particle-level event generators as well as a faithful simulation of the detector, both are in HS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose what is MC(MonteCarlo/simulations) and Data in this context\n",
    "synthetic, nature = datasets['simul'], datasets['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to specify `itnum`: how many iterations of the unfolding procedure we want to do.\n",
    "\n",
    "**Customize**: Change `itnum` to your desired number of unfolding iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many iterations of the unfolding process\n",
    "itnum = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three flavors of OmniFold. In order of increasing sophistication, they are:\n",
    "* **UniFold**: Represent the jet as a single observable.\n",
    "* **MultiFold**: Represent the jet as multiple observables.\n",
    "* **OmniFold**: Represent the jet as a set of particles.\n",
    "\n",
    "i am not shure if this is omnifold or multifold or what is the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase space of the correction\n",
    "obs_multifold = ['Q2', 'Nu', 'Pt2', 'Zh', 'PhiPQ'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observables are already computed in the samples. We will read them in as an observable dictionary `obs` and also specify histogram style information.\n",
    "\n",
    "**Customize**: Add entries to `obs` to define your own observables to be used in MultiFold or to see the unfolding performance on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to hold information about the observables\n",
    "obs = {}\n",
    "\n",
    "# the Q2 and histogram style information (the func is there to add the arrays latter)\n",
    "obs.setdefault('Q2', {}).update({\n",
    "    'func': lambda dset, ptype: dset['Q2_' + ptype],\n",
    "    'nbins_det': 30, 'nbins_mc': 30,\n",
    "    'xlim': (1, 4.2), 'ylim': (0, 1.5),\n",
    "    'xlabel': r'Q2 [$GeV^2$]', 'symbol': r'$Q^2$',\n",
    "    'ylabel': r'Normalized Cross Section ',\n",
    "    'stamp_xy': (0.425, 0.65),\n",
    "})\n",
    "\n",
    "# the Nu and histogram style information\n",
    "obs.setdefault('Nu', {}).update({\n",
    "    'func': lambda dset, ptype: dset['Nu_'+ ptype],\n",
    "    'nbins_det': 30, 'nbins_mc': 30,\n",
    "    'xlim': (2., 5), 'ylim': (0, 1),\n",
    "    'xlabel': r'$\\nu$[$GeV$]', 'symbol': r'$\\nu$',\n",
    "    'ylabel': r'Normalized Cross Section',\n",
    "    'stamp_xy': (0.42, 0.65),\n",
    "})\n",
    "\n",
    "# the Zh and histogram style information\n",
    "obs.setdefault('Zh', {}).update({\n",
    "    'func': lambda dset, ptype: dset['Zh_' + ptype],\n",
    "    'nbins_det': 30, 'nbins_mc':30,\n",
    "    'xlim': (0, 1), 'ylim': (0, 4),\n",
    "    'xlabel': r'$Z_h$', 'symbol': r'$Z_h$',\n",
    "    'ylabel': r'Normalized Cross Section',\n",
    "    'stamp_xy': (0.425, 0.65),\n",
    "})\n",
    "\n",
    "# the Pt2 ratio and histogram style information\n",
    "obs.setdefault('Pt2', {}).update({ \n",
    "    'func': lambda dset, ptype: dset['Pt2_' + ptype],\n",
    "    'nbins_det': 90, 'nbins_mc': 30,\n",
    "    'xlim': (-0.01, 3), 'ylim': (0, 7),\n",
    "    'xlabel': r'$Pt^2$[GeV]', 'symbol': r'$Pt^2$',\n",
    "    'ylabel': r'Normalized Cross Section',\n",
    "    'stamp_xy': (0.41, 0.92),\n",
    "    'legend_loc': 'upper left', 'legend_ncol': 1,\n",
    "})\n",
    "\n",
    "# the PhiPQ fraction and histogram style information\n",
    "obs.setdefault('PhiPQ', {}).update({\n",
    "    'func': lambda dset, ptype: dset['PhiPQ_'+ ptype],\n",
    "    'nbins_det': 30, 'nbins_mc': 30,\n",
    "    'xlim': (-180, 180), 'ylim': (0, 0.01),\n",
    "    'xlabel': r'PhiPQ[Deg]', 'symbol': r'$z_g$',\n",
    "    'ylabel': 'Normalized Cross Section',\n",
    "    'stamp_xy': (0.425, 0.65),\n",
    "})\n",
    "\n",
    "\n",
    "# additional histogram and plot style information\n",
    "hist_style = {'histtype': 'step', 'density': True, 'lw': 1, 'zorder': 2}\n",
    "gen_style = {'linestyle': '-', 'color': 'blue', 'lw': 1.15, 'label': 'Gen.'}\n",
    "truth_style = {'step': 'mid', 'edgecolor': 'green', 'facecolor': (0.75, 0.875, 0.75),\n",
    "               'lw': 1.25, 'zorder': 0, 'label': '``Truth\\\"'}\n",
    "ibu_style = {'ls': '-', 'marker': 'o', 'ms': 2.5, 'color': 'gray', 'zorder': 1}\n",
    "omnifold_style = {'ls': 'dashed', 'marker': 's', 'ms': 2.5, 'color': 'tab:red', 'zorder': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that remains is to get the values of the specified observables and compute the histograms with the specified binnings. As an unfolding benchmark, we also obtain the unfolding results of Iterative Bayesian Unfolding (IBU) as implemented in `ibu.py`. The following cell takes care of all of these aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Q2\n",
      "Done with Nu\n",
      "Done with Zh\n",
      "Done with Pt2\n",
      "Done with PhiPQ\n"
     ]
    }
   ],
   "source": [
    "# calculate quantities to be stored in obs\n",
    "for obkey,ob in obs.items():\n",
    "    \n",
    "    # Add the array with the data to the las dictionary\n",
    "    # calculate observable for GEN, (REC)SIM, DATA, and TRUE\n",
    "    ob['genobs'], ob['simobs'] = ob['func'](synthetic, 'gen'), ob['func'](synthetic, 'rec')\n",
    "    ob['truthobs'], ob['dataobs'] = ob['func'](nature, 'gen'), ob['func'](nature, 'rec')\n",
    "    \n",
    "    # setup bins\n",
    "    # ob['rec/det or gen/mc'] = np.linspace(min val, max val, nbins + 1)\n",
    "    ob['bins_det'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_det']+1)\n",
    "    ob['bins_mc'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_mc']+1)\n",
    "    # = np.linspace(bin-array except the last number, bin-array except the first number) all array vals divided by 2\n",
    "    ob['midbins_det'] = (ob['bins_det'][:-1] + ob['bins_det'][1:])/2\n",
    "    ob['midbins_mc'] = (ob['bins_mc'][:-1] + ob['bins_mc'][1:])/2\n",
    "    # Second val - first val = width\n",
    "    ob['binwidth_det'] = ob['bins_det'][1] - ob['bins_det'][0]\n",
    "    ob['binwidth_mc'] = ob['bins_mc'][1] - ob['bins_mc'][0]\n",
    "    \n",
    "    # get the histograms of GEN, DATA, and TRUTH level observables\n",
    "    #  np.histogram(data Array , binning array, density=True(this normalize the histogram))\n",
    "    # the [0] is to select the first return object of the method,which is the number of event per bin\n",
    "    # in this case normalized beacause density=True\n",
    "    ob['genobs_hist'] = np.histogram(ob['genobs'][ob['genobs'] != dummyval], bins=ob['bins_mc'], density=True)[0]\n",
    "    ob['data_hist'] = np.histogram(ob['dataobs'], bins=ob['bins_det'], density=True)[0]\n",
    "    # modplot.calc_hist(Data array, binning array,  this just select if the bigger or equal of the binning selection \n",
    "    # goes in the right or in the left)\n",
    "    # [:2] this returns the first 2 object created by the method that  is a histgram and the errors \n",
    "    ob['truth_hist'], ob['truth_hist_unc'] = modplot.calc_hist(ob['truthobs'][ob['truthobs'] != dummyval],\n",
    "                                                               bins=ob['bins_mc'], density=True)[:2]\n",
    "\n",
    "    \n",
    "\n",
    "    # compute (and normalize) the response matrix between GEN and SIM\n",
    "    ob['response'] = np.histogram2d(ob['simobs'], ob['genobs'], bins=(ob['bins_det'], ob['bins_mc']))[0]\n",
    "    ob['response'] /= (ob['response'].sum(axis=0) + 10**-50)\n",
    "    \n",
    "    # perform iterative Bayesian unfolding\n",
    "    ob['ibu_phis'] = ibu.ibu(ob['data_hist'], ob['response'], ob['genobs_hist'], \n",
    "                         ob['binwidth_det'], ob['binwidth_mc'], it=itnum)\n",
    "    ob['ibu_phi_unc'] = ibu.ibu_unc(ob, it=itnum, nresamples=25)\n",
    "    \n",
    "    print('Done with', obkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OmniFold\n",
    "\n",
    "Now it's time to set up for the OmniFold procedure and do the unfolding!  \n",
    "\n",
    "Here, we choose model sizes and training parameters that default to a quick training (~5 min). Even with this simplified model and training, we will closely reproduce the full results of the paper.\n",
    "\n",
    "**Customize**: Change the model layer sizes or training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How many layers and neurons for each layer to use in the deep learning\n",
    "model_layer_sizes = [100, 100]\n",
    "# model_layer_sizes = [100, 100, 100] # use this for the full network size\n",
    "\n",
    "# set up the array of data/simulation detector-level observables\n",
    "# X are the features/variables\n",
    "X_det = np.asarray([np.concatenate((obs[obkey]['dataobs'], obs[obkey]['simobs'])) for obkey in obs_multifold]).T\n",
    "# Y are labels(variable that you want to know)/ weights\n",
    "Y_det = ef.utils.to_categorical(np.concatenate((np.ones(len(obs['Q2']['dataobs'])), \n",
    "                                                np.zeros(len(obs['Q2']['simobs'])))))\n",
    "\n",
    "# set up the array of generation particle-level observables\n",
    "X_gen = np.asarray([np.concatenate((obs[obkey]['genobs'], obs[obkey]['genobs'])) for obkey in obs_multifold]).T\n",
    "Y_gen = ef.utils.to_categorical(np.concatenate((np.ones(len(obs['Q2']['genobs'])), \n",
    "                                                np.zeros(len(obs['Q2']['genobs'])))))\n",
    "\n",
    "# standardize the inputs (variables- mean)/standar deviation\n",
    "X_det = (X_det - np.mean(X_det, axis=0))/np.std(X_det, axis=0)\n",
    "X_gen = (X_gen - np.mean(X_gen, axis=0))/np.std(X_gen, axis=0)\n",
    "\n",
    "# Specify the training parameters\n",
    "# model parameters for the Step 1 network\n",
    "det_args = {'input_dim': len(obs_multifold), 'dense_sizes': model_layer_sizes,\n",
    "            'patience': 10, 'filepath': 'Step1_{}', 'save_weights_only': False, \n",
    "            'modelcheck_opts': {'save_best_only': True, 'verbose': 1}}\n",
    "\n",
    "# model parameters for the Step 2 network\n",
    "mc_args = {'input_dim': len(obs_multifold), 'dense_sizes': model_layer_sizes, \n",
    "           'patience': 10, 'filepath': 'Step2_{}', 'save_weights_only': False, \n",
    "           'modelcheck_opts': {'save_best_only': True, 'verbose': 1}}\n",
    "\n",
    "# general training parameters\n",
    "# Batch_size is the number of examples used to train the data at same time(it used all but not all at same time)\n",
    "# epoch i the number of steam\n",
    "# i am not sure about the last two thing hahha\n",
    "fitargs = {'batch_size': 500, 'epochs': 2, 'verbose': 1}\n",
    "#fitargs = {'batch_size': 500, 'epochs': 100, 'verbose': 1} # use this for a full training\n",
    "\n",
    "# reweight the sim and data to have the same total weight to begin with\n",
    "ndata, nsim = np.count_nonzero(Y_det[:,1]), np.count_nonzero(Y_det[:,0])\n",
    "wdata = np.ones(ndata)\n",
    "winit = ndata/nsim*np.ones(nsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.40514725 -0.40514717 -0.4051472  -0.40514734 -0.4023217 ]\n",
      " [-0.40514725 -0.40514717 -0.4051472  -0.40514734 -0.4023217 ]\n",
      " [-0.40514725 -0.40514717 -0.4051472  -0.40514734 -0.4023217 ]\n",
      " ...\n",
      " [-0.40514725 -0.40514717 -0.4051472  -0.40514734 -0.4023217 ]\n",
      " [-0.40514725 -0.40514717 -0.4051472  -0.40514734 -0.4023217 ]\n",
      " [ 2.466508    2.465831    2.4690807   2.469681    2.8457773 ]]\n"
     ]
    }
   ],
   "source": [
    "#print(X_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `omnifold` method within `omnifold.py` takes all the relevant information and performs the unfolding process for the specified number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 15:12:20.364746: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-10-27 15:12:20.364878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (matias): /proc/driver/nvidia/version does not exist\n",
      "2022-10-27 15:12:20.366767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 5)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               600       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,902\n",
      "Trainable params: 10,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 5)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               600       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,902\n",
      "Trainable params: 10,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.7101 - acc: 0.5028WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69490, saving model to Step1_0_Epoch-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 5s 105ms/step - loss: 0.7085 - acc: 0.5021 - val_loss: 0.6949 - val_acc: 0.4888\n",
      "Epoch 2/2\n",
      "30/32 [===========================>..] - ETA: 0s - loss: 0.6942 - acc: 0.5057WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.69490 to 0.69322, saving model to Step1_0_Epoch-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Step1_0_Epoch-2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 93ms/step - loss: 0.6942 - acc: 0.5068 - val_loss: 0.6932 - val_acc: 0.5138\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "# apply the OmniFold procedure to get weights for the generation\n",
    "multifold_ws = omnifold.omnifold(X_gen, Y_gen, X_det, Y_det, wdata, winit,\n",
    "                                (ef.archs.DNN, det_args), (ef.archs.DNN, mc_args),\n",
    "                                fitargs, val=0.2, it=itnum, trw_ind=-2, weights_filename='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Unfolding Results\n",
    "\n",
    "Now it's time to plot the unfolding results for all of the specified observables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_directory  = \"/home/matias/proyecto/Omnifold/Plots/\"\n",
    "\n",
    "for i,(obkey,ob) in enumerate(obs.items()):\n",
    "\n",
    "\n",
    "    # get the styled axes on which to plot\n",
    "    fig, [ax0, ax1] = modplot.axes(**ob)\n",
    "    if ob.get('yscale') is not None:\n",
    "        ax0.set_yscale(ob['yscale'])\n",
    "\n",
    "        \n",
    "    # Plot the Different Distributions of the Observable\n",
    "    # plot the \"data\" histogram of the observable\n",
    "    \n",
    "    ax0.hist(ob['dataobs'][ob['dataobs'] != dummyval], bins=ob['bins_det'], color='black', label='\\\"Data\\\"', **hist_style)\n",
    "\n",
    "\n",
    "    # plot the \"sim\" histogram of the observable\n",
    "    ax0.hist(ob['simobs'], bins=ob['bins_det'], color='orange', label='Sim.', **hist_style, linestyle = 'dotted')\n",
    "\n",
    "    # plot the \"gen\" histogram of the observable\n",
    "    ax0.plot(ob['midbins_mc'], ob['genobs_hist'], **gen_style)\n",
    "\n",
    "    # plot the \"truth\" histogram of the observable\n",
    "    ax0.fill_between(ob['midbins_mc'], ob['truth_hist'], **truth_style)\n",
    "\n",
    "    \n",
    "    # Plot the Unfolded Distributions of the Observable\n",
    "    # plot the OmniFold distribution\n",
    "    of_histgen, of_histgen_unc = modplot.calc_hist(ob['genobs'][ob['genobs'] != dummyval], \n",
    "                                                   weights=multifold_ws[2*itnum-1][ob['genobs'] != dummyval], \n",
    "                                                   bins=ob['bins_mc'], density=True)[:2]\n",
    "    \n",
    "    ax0.plot(ob['midbins_mc'], of_histgen, **omnifold_style, label='MultiFold')\n",
    "\n",
    "    # plot the IBU distribution\n",
    "    #ax0.plot(ob['midbins_mc'], ob['ibu_phis'][itnum], **ibu_style, label='IBU ' + ob['symbol'])\n",
    "\n",
    "    # Plot the Ratios of the OmniFold and IBU distributions to truth (with statistical uncertainties)\n",
    "    # ibu_ratio = ob['ibu_phis'][itnum]/(ob['truth_hist'] + 10**-50)\n",
    "    of_ratio = of_histgen/(ob['truth_hist'] + 10**-50)\n",
    "    ax1.plot([np.min(ob['midbins_mc']), np.max(ob['midbins_mc'])], [1, 1], '-', color='green', lw=0.75)\n",
    "    \n",
    "    # ratio uncertainties\n",
    "    truth_unc_ratio = ob['truth_hist_unc']/(ob['truth_hist'] + 10**-50)\n",
    "    #ibu_unc_ratio = ob['ibu_phi_unc']/(ob['truth_hist'] + 10**-50)\n",
    "    of_unc_ratio = of_histgen_unc/(ob['truth_hist'] + 10**-50)\n",
    "    \n",
    "    ax1.fill_between(ob['midbins_mc'], 1 - truth_unc_ratio, 1 + truth_unc_ratio, \n",
    "                     facecolor=truth_style['facecolor'], zorder=-2)\n",
    "    #ax1.errorbar(ob['midbins_mc'], ibu_ratio, xerr=ob['binwidth_mc']/2, yerr=ibu_unc_ratio, \n",
    "                                              #color=ibu_style['color'], **modplot.style('errorbar'))\n",
    "    ax1.errorbar(ob['midbins_mc'], of_ratio, xerr=ob['binwidth_mc']/2, yerr=of_unc_ratio, \n",
    "                                              color=omnifold_style['color'], **modplot.style('errorbar'))\n",
    "\n",
    "    # legend style and ordering\n",
    "    loc, ncol = ob.get('legend_loc', 'upper right'), ob.get('legend_ncol', 2)\n",
    "    order = [3, 4, 2, 0, 1] if ncol==2 else [3, 4, 0, 2, 1]\n",
    "    modplot.legend(ax=ax0, frameon=False, order=order, loc=loc, ncol=ncol)\n",
    "\n",
    "\n",
    "    # save plot.\n",
    "    fig.savefig(plot_directory + 'MultiFold_{}.pdf'.format(obkey), bbox_inches='tight')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dic = {'weight': multifold_ws[2*itnum]}\n",
    "data.update(weight_dic)\n",
    "print(data['weight'][data['weight']>1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with uproot.recreate(\"/home/matias/proyecto/Omnifold/Data/data_weights_\" + target + \".root\") as output_file:\n",
    "    output_file['ntuple_pion'] = data\n",
    "    output_file['ntuple_pion'].show()\n",
    "    output_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
